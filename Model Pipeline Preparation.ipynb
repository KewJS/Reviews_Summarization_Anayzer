{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "correct-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import torch\n",
    "from summarizer import Summarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from src.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-occasion",
   "metadata": {},
   "source": [
    "## Load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "requested-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine(\"sqlite:///\" + Config.FILES[\"DATABASE_DIR\"])\n",
    "df = pd.read_sql_table(\"Text_table\", engine)\n",
    "\n",
    "# display loaded dataframe\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-system",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "immediate-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kewjs\\Documents\\02-Self_Learning\\01-Data_Science\\12-News_Summarizer\\summarizer_venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "weekly-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_sum(text, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Generate extractive summary using BERT model\n",
    "    \n",
    "    INPUT:\n",
    "    text - str. Input text\n",
    "    ratio - float. Enter a ratio between 0.1 - 1.0 [default = 0.8]\n",
    "            (ratio = summary length / original text length)\n",
    "    \n",
    "    OUTPUT:\n",
    "    summary - str. Generated summary\n",
    "    \"\"\"\n",
    "    bert_model = Summarizer()\n",
    "    summary = bert_model(text, ratio=ratio)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def abs_sum(text, model, tokenizer, min_length=80, \n",
    "                     max_length=150, length_penalty=15, \n",
    "                     num_beams=2):\n",
    "    \"\"\"\n",
    "    Generate abstractive summary using T5 model\n",
    "    \n",
    "    INPUT:\n",
    "    text - str. Input text\n",
    "    model - model name\n",
    "    tokenizer - model tokenizer\n",
    "    min_length - int. The min length of the sequence to be generated\n",
    "                      [default = 80]\n",
    "    max_length - int. The max length of the sequence to be generated \n",
    "                      [default = 150]\n",
    "    length_penalty - float. Set to values < 1.0 in order to encourage the model \n",
    "                     to generate shorter sequences, to a value > 1.0 in order to \n",
    "                     encourage the model to produce longer sequences.\n",
    "                     [default = 15]\n",
    "    num_beams - int. Number of beams for beam search. 1 means no beam search\n",
    "                     [default = 2]\n",
    "    \n",
    "    OUTPUT:\n",
    "    summary - str. Generated summary\n",
    "    \"\"\"\n",
    "    tokens_input = tokenizer.encode(\"summarize: \"+text, return_tensors='pt', \n",
    "                                    # model tokens max input length\n",
    "                                    max_length=tokenizer.model_max_length, \n",
    "                                    truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(tokens_input,\n",
    "                                min_length=min_length,\n",
    "                                max_length=max_length,\n",
    "                                length_penalty=length_penalty, \n",
    "                                num_beams=num_beams)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_summary(text, model, tokenizer, ext_ratio=1.0, min_length=80, \n",
    "                     max_length=150, length_penalty=15, \n",
    "                     num_beams=2):\n",
    "    \"\"\"\n",
    "    Generate summary for using extractive & abstractive methods\n",
    "    \n",
    "    INPUT:\n",
    "    text - str. Input text\n",
    "    model - model name\n",
    "    tokenizer - model tokenizer\n",
    "    ext_ratio - float. Enter a ratio between 0.1 - 1.0 [default = 1.0]\n",
    "                (ratio = summary length / original text length)\n",
    "                1.0 means no extractive summarization is performed before \n",
    "                abstractive summarization\n",
    "    min_length - int. The min length of the sequence to be generated\n",
    "                 [default = 80]\n",
    "    max_length - int. The max length of the sequence to be generated \n",
    "                 [default = 150]\n",
    "    length_penalty - float. Set to values < 1.0 in order to encourage the model \n",
    "                     to generate shorter sequences, to a value > 1.0 in order to \n",
    "                     encourage the model to produce longer sequences.\n",
    "                     [default = 15]\n",
    "    num_beams - int. Number of beams for beam search. 1 means no beam search\n",
    "                     [default = 2]\n",
    "    \n",
    "    OUTPUT:\n",
    "    summary - str. Generated summary\n",
    "    \"\"\"\n",
    "    if ext_ratio == 1.0:\n",
    "        summary = abs_sum(text, model, tokenizer, min_length, \n",
    "                       max_length, length_penalty, num_beams)\n",
    "    elif ext_ratio < 1.0:\n",
    "        text = ext_sum(text, ratio = ext_ratio)\n",
    "        summary = abs_sum(text, model, tokenizer, min_length, \n",
    "                       max_length, length_penalty, num_beams)\n",
    "    else:\n",
    "        print('Error! Please enter ext_ratio betwen 0.1 and 1.0')\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "powered-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sum_save_monitor(df, model, tokenizer, output_folder, ext_ratio=1.0, \n",
    "                         min_length=80, max_length=150, length_penalty=15, \n",
    "                         num_beams=2):\n",
    "    \"\"\"\n",
    "    Monitor progress while generating summary & save output to list & text file\n",
    "    \n",
    "    INPUT:\n",
    "    df - DataFrama. Data loaded from database\n",
    "    model - model name\n",
    "    tokenizer - model tokenizer\n",
    "    output_folder - str. Folder name to save the generated output in text file\n",
    "    ext_ratio - float. Enter a ratio between 0.1 - 1.0 [default = 1.0]\n",
    "                (ratio = summary length / original text length)\n",
    "                1.0 means no extractive summarization is performed before \n",
    "                abstractive summarization\n",
    "    min_length - int. The min length of the sequence to be generated\n",
    "                 [default = 80]\n",
    "    max_length - int. The max length of the sequence to be generated\n",
    "                 [default = 150]\n",
    "    length_penalty - float. Set to values < 1.0 in order to encourage the model \n",
    "                     to generate shorter sequences, to a value > 1.0 in order to \n",
    "                     encourage the model to produce longer sequences.\n",
    "                     [default = 15]\n",
    "    num_beams - int. Number of beams for beam search. 1 means no beam search \n",
    "                [default = 2]\n",
    "    \n",
    "    OUTPUT:\n",
    "    summaries - list. Generated summary appended to a list\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for i in range(len(df)):\n",
    "        file_path = df.file_path[i]\n",
    "        raw_text = df.raw_text[i]\n",
    "    \n",
    "        start = time.time()\n",
    "        summary = generate_summary(raw_text, model, tokenizer, \n",
    "                                   ext_ratio, min_length, max_length, \n",
    "                                   length_penalty, num_beams)\n",
    "        \n",
    "        file_name = file_path[4:][:-4]+'_summary.txt'\n",
    "        \n",
    "        with open(output_folder + \"/\" + file_name, 'w')as text_file:\n",
    "            text_file.write(summary)\n",
    "        \n",
    "        \n",
    "        summaries.append(summary)\n",
    "        end = time.time()\n",
    "        print(\" Summarized '{}'[time: {:.2f}s]\".format(file_path, \n",
    "                                                       end-start))\n",
    "        \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seasonal-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summarized 'circle-of-life-hospice.pdf'[time: 8.59s]\n",
      " Summarized 'Concord Regional VNA Systems Success Story.pdf'[time: 8.14s]\n",
      " Summarized 'first-choice-home-health-and-hospice.pdf'[time: 7.99s]\n",
      " Summarized 'Maple Knoll Communities success story.pdf'[time: 8.57s]\n",
      " Summarized 'willow-health.pdf'[time: 8.35s]\n"
     ]
    }
   ],
   "source": [
    "summaries = gen_sum_save_monitor(df, model, tokenizer, output_folder=Config.FILES[\"PREPROCESS_DIR\"], ext_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spare-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"summary\"] = summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "isolated-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: pdf/Maple Knoll Communities success story.pdf\n",
      "\n",
      "Summary:\n",
      "---------\n",
      "Maple Knoll Communities has been delivering care for over 172 years. the organization is by no means encumbered by processes of the past. Maple Knoll believes in the power of innovation and that technology can, and does, make life easier for residents, staff and families. with Netsmart Telehealth, Maple Knoll can improve clinical satisfaction. with virtual visits and improve physician. & resident satisfaction. the organization has been delivering care for over 172 years, the organization is by no means \n",
      "\n",
      "\n",
      "File path: pdf/circle-of-life-hospice.pdf\n",
      "\n",
      "Summary:\n",
      "---------\n",
      "Circle of Life Hospice is the largest non-profit hospice in northwest Arkansas. Circle of Life is committed to compassionate end-of-life care for a person’s body, mind, spirit and family when there is no longer a cure. the hospital readmission rate for patients admitted to Circle of Life in 2017 was 0.5 percent. the organization’s core values of compassion, inclusion, respect, comfort, leadership and excellence make Circle of Life a community of quality care when it matters most. the hospice advisor solution was a\n",
      "\n",
      "\n",
      "File path: pdf/Concord Regional VNA Systems Success Story.pdf\n",
      "\n",
      "Summary:\n",
      "---------\n",
      "a Concord regional VNA (CRVNA) in central new hampshire has 378 staff members offering home care, hospice, palliative care. the referral process accounts for a significant portion of a homecare organization’s business— 100 percent to be exact. referring agencies and especially referring physicians must:  Understand the agency’s strengths and capabilities  Have confidence in the level of care their patients will receive.  Establish efficient communication and data exchange methods that enable and facilitate future care plan\n",
      "\n",
      "\n",
      "File path: pdf/willow-health.pdf\n",
      "\n",
      "Summary:\n",
      "---------\n",
      "Netsmart EHR delivered a fully-integrated single patient record that unifies clinical, financial and census data across the full continuum of care. the solution was selected for its ability to deliver a fully-integrated single patient record that unifies clinical, financial and census data across the full continuum of care. 'it is well worth the money to spend time with the experts at Netsmart to review what we use, how we use it, and how we can do better,' says Shelly Miller chief\n",
      "\n",
      "\n",
      "File path: pdf/first-choice-home-health-and-hospice.pdf\n",
      "\n",
      "Summary:\n",
      "---------\n",
      "first choice home health and hospice was clear about its need for a fully electronic homecare and hospice solution. \"we wanted an innovator—someone who would always push forward,\" said Beau Sorensen, chief financial officer. \"we're seeing a 17 percent increase in clinician productivity,\" said Sorensen. \"it's a great time to be a part of a company that is transforming itself,\" said sorensen.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the generated summary\n",
    "for i in range(len(df)):\n",
    "    print(\"File path: {}\".format(df.file_path[i]))\n",
    "    print(\"\")\n",
    "    print('Summary:')\n",
    "    print(\"---------\")\n",
    "    print(df.summary[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "centered-tours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(\"sqlite:///\" + Config.FILES[\"DATABASE_DIR\"])\n",
    "df.to_sql(\"Text_table\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
